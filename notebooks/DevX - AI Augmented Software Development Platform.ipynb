{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7e84064",
   "metadata": {},
   "source": [
    "\n",
    "# DevX -  AI Augmented Software Development Platform  (Ashish K Jain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae621ee4",
   "metadata": {},
   "source": [
    "## Multi Agent Event Driven Software Development platform using LlamaIndex and OpenAI’s GPT-4o & O3-Mini\"\n",
    "\n",
    "### DevX is transforming the way engineering teams build, debug, and optimize software. This notebook demonstrates an AI-powered development workflow that leverages LlamaIndex to coordinate two advanced models: GPT-4o-mini for analyzing user input, and O3-Mini for tasks such as design, coding, DevOps, and more.\n",
    "\n",
    "### Key Features - Purpose: \n",
    "    Multi-agent, event-driven architecture.\n",
    "    AI-assisted generation of user stories, designs, code, DevOps configurations, and test cases.\n",
    "    Feedback-driven refinement at every stage of the software development lifecycle.\n",
    "\n",
    "### While AI-assisted coding is becoming increasingly common, this notebook introduces a unique event-driven, agentic workflow. Engineers can initiate the development process with a simple requirement, and DEVX intelligently guides them through each phase of the software development lifecycle—including user stories, system design, code, DevOps integration, and test case generation.\n",
    "\n",
    "### At every stage, engineers can provide feedback to refine and adjust the output according to their needs. There's no need for manual prompt engineering—developers can simply offer feedback to regenerate content, proceed to the next step, or stop the workflow at any point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c840ca91",
   "metadata": {},
   "source": [
    "## DevX WorkFlow\n",
    "<img src=\"../images/DevX-Flow.png\" alt=\"DevX WorkFlow\" width=\"1000\" height=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468f6302",
   "metadata": {},
   "source": [
    "#### This cell lists the required Python libraries for the notebook. It includes commands to install OpenAI, Gradio, and various LlamaIndex-related packages. These libraries are essential for interacting with OpenAI models, building workflows, and creating a user interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1edeeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install openai\n",
    "#pip install gradio\n",
    "#pip install llama-parse\n",
    "#pip install llama-index-utils-workflow\n",
    "#pip install llama-index-core "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4777de2",
   "metadata": {},
   "source": [
    "#### This cell imports the necessary Python libraries and modules. It includes OpenAI for API interaction, LlamaIndex for workflow management, and utility functions for visualizing workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b70344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from llama_index.core.workflow import (\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Event,\n",
    "    Context\n",
    ")\n",
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "from llama_index.core.workflow import InputRequiredEvent, HumanResponseEvent\n",
    "import gradio as gr\n",
    "import asyncio\n",
    "import threading\n",
    "import queue\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc652cf0",
   "metadata": {},
   "source": [
    "#### We must load the OpenAI key from the environment variable and set it into api_key. We will get this key while registering with OPENAI. We then instantiate the Open AI client using the key. We will also define two variables for two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d31d568",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key  = os.getenv('OPENAI_API_KEY')\n",
    "client = OpenAI(api_key=openai.api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077f5ad9",
   "metadata": {},
   "source": [
    "#### This cell applies nest_asyncio to allow nested asynchronous loops, which is necessary for running asynchronous workflows in a Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c36384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec5be20",
   "metadata": {},
   "source": [
    "#### This function interacts with the OpenAI model to generate responses based on a given prompt and model type. It simplifies the process of querying the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75175c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(model, prompt, model_type):\n",
    "    model_response = model.chat.completions.create(model=model_type,\n",
    "                                                     messages=[{\"role\":\"user\",\"content\": prompt}])\n",
    "    response_content = model_response.choices[0].message.content\n",
    "    return response_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6ba226",
   "metadata": {},
   "source": [
    "####  This section defines custom event classes for different stages of the workflow, such as ProductManagerEvent, SoftwareArchitectEvent, and DevOpsEngineerEvent. These classes encapsulate the data required for each stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4279ad52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductManagerEvent(Event):\n",
    "    requirement: str\n",
    "\n",
    "class UserStoryFeedbackEvent(Event):\n",
    "    feedback: str\n",
    "        \n",
    "class SoftwareArchitectEvent(Event):\n",
    "    userStories: str\n",
    "\n",
    "class DesignFeedbackEvent(Event):\n",
    "    feedback: str\n",
    "\n",
    "class SoftwareEngineerEvent(Event):\n",
    "    design: str\n",
    "\n",
    "class CodeFeedbackEvent(Event):\n",
    "    feedback: str \n",
    "\n",
    "class DevOpsEngineerEvent(Event):\n",
    "    reviewedCode: str\n",
    "\n",
    "class DevOpsFeedbackEvent(Event):\n",
    "    feedback: str \n",
    "\n",
    "\n",
    "class TestEngineerEvent(Event):\n",
    "    devopscode: str\n",
    "\n",
    "class TestCaseFeedbackEvent(Event):\n",
    "    feedback: str \n",
    "        \n",
    "class ProgressEvent(Event):\n",
    "    msg: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f09e232",
   "metadata": {},
   "source": [
    "#### This section implements the AIDevXWorkflow class, which defines the steps of the AI-driven development workflow. Each step is annotated with @step and performs a specific task, such as generating user stories, accepting feedback, or progressing to the next stage.\n",
    "\n",
    "#### Key Steps:\n",
    "    set_up: Initializes the workflow and validates user requirements.\n",
    "    generateUserStories: Generates user stories based on requirements.\n",
    "    get_userStories_feedback: Processes user feedback on user stories.\n",
    "    generateDesign: Generates a high-level system design.\n",
    "    get_design_feedback: Processes user feedback on the design.\n",
    "    generateCode: Generates code based on the design.\n",
    "    get_code_feedback: Processes user feedback on the code.\n",
    "    generateDevOps: Generates DevOps configurations.\n",
    "    get_devOps_code_feedback: Processes user feedback on DevOps configurations.\n",
    "    generateTestCases: Generates test cases for the system.\n",
    "    get_testcase_feedback: Processes user feedback on test cases.\n",
    "\n",
    "#### Right now, each step in the DevX flow is powered by direct OpenAI calls through LlamaIndex. But you can think of them as early-stage agents—each designed with a specific function and responsibility. With RAG and persistent memory, these will evolve into intelligent AI agents that reason across stages, recall past decisions, and collaborate like real teammates.”*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1f0702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The AIDevXWorkflow class is a workflow automation framework designed to guide the software development \n",
    "# lifecycle using AI-driven steps. It leverages OpenAI models to generate user stories, designs, code, \n",
    "# DevOps configurations, and test cases while incorporating user feedback at each stage.\n",
    "\n",
    "class AIDevXWorkflow(Workflow):\n",
    "    \n",
    "    GPT_4O_MINI_MODEL = 'gpt-4o-mini'\n",
    "    O3_MINI_MODEL = 'o3-mini'\n",
    "    model: OpenAI\n",
    "    \n",
    "    # declare a function as a step\n",
    "    @step\n",
    "    async def set_up(self, ctx: Context, ev: StartEvent) -> ProductManagerEvent:\n",
    "        \n",
    "        # Check for if developers requirement\n",
    "        if not ev.query:\n",
    "            raise ValueError(\"User Requirements are missing\")\n",
    "        \n",
    "        self.model = OpenAI(api_key=openai.api_key)\n",
    "        await ctx.set(\"requirement\", str(ev.query))\n",
    "        pevent = ProgressEvent(\n",
    "            msg=\"Workflow started. Generating User-Stories\",\n",
    "            event_type=\"GENERATE_USER_STORIES\"\n",
    "        )\n",
    "    \n",
    "        ctx.write_event_to_stream(pevent)\n",
    "        \n",
    "        return ProductManagerEvent(requirement=ev.query)\n",
    "    \n",
    "    @step\n",
    "    async def generateUserStories(self, ctx: Context, ev: ProductManagerEvent | UserStoryFeedbackEvent) -> InputRequiredEvent:\n",
    "        requirement = await ctx.get(\"requirement\")\n",
    "        user_feedback_on_users_stories = \"\"\n",
    "        if hasattr(ev,\"feedback\"):\n",
    "            user_feedback_on_users_stories = ev.feedback\n",
    "        \n",
    "        product_manager_agent = f\"\"\"You are an experienced Software Product Manager responsible for converting \n",
    "            high-level business requirements into detailed user stories. Your output will be used by the Software Architect \n",
    "            to propose a high-level design and technology stack.\n",
    "\n",
    "            Instructions:\n",
    "\n",
    "            1-) Analyze the given high-level requirements and break them down into well-defined user stories using the INVEST criteria (Independent, Negotiable, Valuable, Estimable, Small, Testable).\n",
    "            Each user story should follow the standard format:\n",
    "            2-) \"As a [user role], I want [functionality] so that [benefit].\"\n",
    "            3-) Include Acceptance Criteria for each user story using the Gherkin format (Given-When-Then).\n",
    "            4-) Provide any assumptions or constraints that should be considered.\n",
    "            5-) Please also mention the user story number on each story so we can refer that with these numbers.\n",
    "\n",
    "            BELOW ARE THE REQUIREMENTS\n",
    "            {requirement}\n",
    "\n",
    "            You might have generated user stories before and user might have given some feedback for the same. If USER INSTRUCTIONS is\n",
    "            empty assume its first time if not then follow the feedback for regeneration.\n",
    "\n",
    "            USER INSTRUCTIONS\n",
    "            {user_feedback_on_users_stories}\n",
    "\n",
    "            \"\"\"\n",
    "        userStories = call_model(self.model,product_manager_agent,self.O3_MINI_MODEL)\n",
    "        \n",
    "        \n",
    "        # Save the user stories for later\n",
    "        await ctx.set(\"userStories\", str(userStories))\n",
    "        \n",
    "        pevent = ProgressEvent(\n",
    "            msg=\"User Stories generated successfully\",\n",
    "            event_type=\"userstory_feedback\"\n",
    "        )\n",
    "        ctx.write_event_to_stream(pevent)\n",
    "        # Let's get a human in the loop\n",
    "        return InputRequiredEvent(\n",
    "            prefix=\"\\n\\n\"+\"Above are the user-stories. Do you have any feedback please let us know. You can also choosed 1 , 2 or more storeis for generate code\",\n",
    "            result=userStories,\n",
    "            event_type=\"userstory_feedback\"\n",
    "        )\n",
    "\n",
    "    \n",
    "    \n",
    "    @step\n",
    "    async def get_userStories_feedback(self, ctx: Context, ev: HumanResponseEvent) -> UserStoryFeedbackEvent | SoftwareArchitectEvent | StopEvent:\n",
    "        if (ev.event_type == \"userstory_feedback\"):\n",
    "            response = ev.response\n",
    "            feedback = f\"\"\"\n",
    "                You have received some human feedback on the user stories. User either can give some feedback and ask\n",
    "                for the regeneration user stories again or can pick one or more stories for design generation.\n",
    "                <feedback>\n",
    "                {response}\n",
    "                </feedback>\n",
    "                If there's any feedback on user stories and ask for regenration, respond with just the word 'FEEDBACK'.\n",
    "                If ask for go for design generation for one or more stories say 'OKAY\n",
    "                Any If ask for finish or any other thing say 'CLOSE'.\n",
    "\n",
    "            \"\"\"\n",
    "            result = call_model(self.model,feedback,self.GPT_4O_MINI_MODEL)\n",
    "\n",
    "            verdict = result.strip() \n",
    "            \n",
    "            if (verdict == \"OKAY\"):\n",
    "                pevent = ProgressEvent(\n",
    "                    msg=f\"User has provided update on the user stories OK. Generating Design\",\n",
    "                    event_type=\"GENERATE_DESIGN\"\n",
    "                )\n",
    "                ctx.write_event_to_stream(pevent)\n",
    "                await ctx.set(\"user_instruction_for_design_generation\", str(response))\n",
    "                return SoftwareArchitectEvent(userStories = await ctx.get(\"userStories\"))\n",
    "            if (verdict == \"CLOSE\"):\n",
    "                pevent = ProgressEvent(\n",
    "                    msg=f\"User want to exit the workflow. Stopping the Workflow. Please start new workflow by submitting new requirement\",\n",
    "                    event_type=\"STOP\"\n",
    "                )\n",
    "                ctx.write_event_to_stream(pevent)\n",
    "                return StopEvent(result = \"Excellent you have got what you need. Best of luck for your product\")\n",
    "            else:\n",
    "                pevent = ProgressEvent(\n",
    "                    msg=f\"User has provided feedback on the user stories. So Generating User story again with feedback\",\n",
    "                    event_type=\"GENERATE_USER_STORIES\"\n",
    "                )\n",
    "                ctx.write_event_to_stream(pevent)\n",
    "                return UserStoryFeedbackEvent(feedback=ev.response)\n",
    "    \n",
    "    \n",
    "    @step\n",
    "    async def generateDesign(self, ctx: Context, ev: SoftwareArchitectEvent | DesignFeedbackEvent) -> InputRequiredEvent:\n",
    "        requirement = await ctx.get(\"requirement\")\n",
    "        userStories = await ctx.get(\"userStories\")\n",
    "        user_instruction_for_design_generation = await ctx.get(\"user_instruction_for_design_generation\")\n",
    "        user_feedback_on_design = \"\"\n",
    "        if hasattr(ev,\"feedback\"):\n",
    "            user_feedback_on_design = ev.feedback\n",
    "    \n",
    "        ai_architect_prompt = f\"\"\"You are an experienced Software Architect, responsible for designing a scalable, high-performance \n",
    "            based on the provided user stories. Your output will serve as input for the AI Engineer agent, which will \n",
    "            implement the system based on your design.\n",
    "\n",
    "            Your task - \n",
    "\n",
    "            1-) Define the high-level architecture, specifying key components and their responsibilities.\n",
    "            2-) Recommend the technology stack, considering factors like scalability, performance, and maintainability.\n",
    "            3-) Design the data flow, if any\n",
    "            4-) Specify databases and caching mechanisms (e.g., PostgreSQL, Redis) and justify their use. If any\n",
    "            5-) Outline inter-service communication (e.g., REST APIs).\n",
    "            6-) Include security considerations and best practices for reliability and fault tolerance.\n",
    "            7-) You also need to check if their is need to built any new service or component or existing component can help \n",
    "            the design.\n",
    "\n",
    "            You will given all the user-storeis for system and user instruction which one he want to design, it might include one, \n",
    "            two or all user stories. You will also additionaly given initial requirements on which these user stories has been \n",
    "            drived which might help understand the full picture.\n",
    "\n",
    "            REQUIREMENTS\n",
    "            {requirement}\n",
    "\n",
    "            ALL USER STORIES\n",
    "            {userStories}\n",
    "\n",
    "            USER INSTRUCTIONS_FOR_DESIGN_GENERATION\n",
    "            {user_instruction_for_design_generation}\n",
    "            \n",
    "            You might have generated design before and user might have given some feedback for the same. If USER INSTRUCTIONS is\n",
    "            empty assume its first time if not then follow the feedback for regeneration.\n",
    "\n",
    "            USER INSTRUCTIONS_ON_DESIGN_REGERATION\n",
    "            {user_feedback_on_design}\n",
    "\n",
    "            \"\"\"\n",
    "        \n",
    "        design = call_model(self.model,ai_architect_prompt,self.O3_MINI_MODEL)\n",
    "        # Save the user stories for later\n",
    "        await ctx.set(\"design\", str(design))\n",
    "        \n",
    "        pevent = ProgressEvent(\n",
    "            msg=\"Design generated successfully\",\n",
    "            event_type=\"design_feedback\"\n",
    "        )\n",
    "        ctx.write_event_to_stream(pevent)\n",
    "        #Let's get a human in the loop\n",
    "        return InputRequiredEvent(\n",
    "            prefix=\"\\n\\n\"+\"Above are the design for selected user-story . Do you have any feedback please let us know or You can go for code generation\",\n",
    "            result=design,\n",
    "            event_type=\"design_feedback\"\n",
    "        )\n",
    "    \n",
    "    \n",
    "    \n",
    "    @step\n",
    "    async def get_design_feedback(self, ctx: Context, ev: HumanResponseEvent) -> DesignFeedbackEvent | SoftwareEngineerEvent | StopEvent:\n",
    "        if (ev.event_type == \"design_feedback\"):\n",
    "            response = ev.response\n",
    "            feedback = f\"\"\"\n",
    "                You have received some human feedback on the design. User either can give some feedback and ask\n",
    "                for the regeneration design again or ask for the next step that is code generation for design.\n",
    "                <feedback>\n",
    "                {response}\n",
    "                </feedback>\n",
    "                If there's any feedback on design and ask for regenration, respond with just the word 'FEEDBACK'.\n",
    "                If ask for go for code generation or ask for next step say 'OKAY'.\n",
    "                Any If ask for finish or any other thing say 'CLOSE'.\n",
    "\n",
    "            \"\"\"\n",
    "            result = call_model(self.model,feedback,self.GPT_4O_MINI_MODEL)\n",
    "\n",
    "            verdict = result.strip()\n",
    "            \n",
    "            if (verdict == \"OKAY\"):\n",
    "                pevent = ProgressEvent(\n",
    "                    msg=f\"User has provided update on the Design OK. Generating Code\",\n",
    "                    event_type=\"GENERATE_CODE\"\n",
    "                )\n",
    "                ctx.write_event_to_stream(pevent)\n",
    "                await ctx.set(\"user_instruction_for_code_generation\", str(response))\n",
    "                return SoftwareEngineerEvent(design= await ctx.get(\"design\"))\n",
    "            if (verdict == \"CLOSE\"):\n",
    "                pevent = ProgressEvent(\n",
    "                    msg=f\"User want to exit the workflow. Stopping the Workflow. Please start new workflow by submitting new requirement\",\n",
    "                    event_type=\"STOP\"\n",
    "                )\n",
    "                ctx.write_event_to_stream(pevent)\n",
    "                return StopEvent(result = \"Excellent you have got what you need. Best of luck for your product\")\n",
    "            else:\n",
    "                pevent = ProgressEvent(\n",
    "                    msg=f\"User has provided feedback on the Design. So Generating Design again with feedback\",\n",
    "                    event_type=\"GENERATE_DESIGN\"\n",
    "                )\n",
    "                ctx.write_event_to_stream(pevent)\n",
    "                return DesignFeedbackEvent(feedback=ev.response)\n",
    "    \n",
    "    \n",
    "    \n",
    "    @step\n",
    "    async def generateCode(self, ctx: Context, ev: SoftwareEngineerEvent | CodeFeedbackEvent) -> InputRequiredEvent:\n",
    "        userStories = await ctx.get(\"userStories\")\n",
    "        design = await ctx.get(\"design\")\n",
    "        user_instruction_for_code_generation = ctx.get(\"user_instruction_for_code_generation\")\n",
    "        user_feedback_on_code = \"\"\n",
    "        if hasattr(ev,\"feedback\"):\n",
    "            user_feedback_on_code = ev.feedback\n",
    "        \n",
    "        ai_engineer_prompt = f\"\"\"\n",
    "            You are an experienced AI engineer responsible for implementing a software design into a working codebase. \n",
    "            Given a AI ARCHITETCT SYSTEM DESIGN RESPONSE, your task is to:\n",
    "\n",
    "            1-) Generate clean, modular, and well-documented code following the given architecture.\n",
    "            2-) Provide step-by-step setup instructions to help developers run the system.\n",
    "            3-) Ensure security, scalability, and maintainability in the implementation.\n",
    "\n",
    "            Instructions for Code Implementation - \n",
    "\n",
    "            Read the provided software design carefully, understanding its architecture, \n",
    "            components, data flow, and technology stack. You also need to intelligently find out what needs to be build \n",
    "            and any recommendation for AI Engineer.\n",
    "\n",
    "            Generate the necessary code for each module, ensuring:\n",
    "\n",
    "            1-) Clear separation of concerns (e.g., API layer, business logic, data access).\n",
    "            2-) Proper validation, error handling, and logging.\n",
    "            3-) Adherence to security best practices (e.g., authentication, encryption, rate limiting).\n",
    "            4-) Scalability and performance optimizations.\n",
    "            5-) Include comments and documentation for maintainability.\n",
    "            6-) Please dont generate any code for infra or DevOps as we will have next step for the same.\n",
    "\n",
    "            Generate configuration files, if required (e.g., environment variables, database schemas, API keys).\n",
    "\n",
    "            Ensure inter-service communication is correctly implemented (e.g., REST APIs, message queues, database queries).\n",
    "\n",
    "            You will also additionaly given all user stories which might help understand the full picture and design for user \n",
    "            selected user stories. You will also have instruction for code generation if any.\n",
    "\n",
    "            ALL USER STORIES\n",
    "            {userStories}\n",
    "\n",
    "            AI ARCHITETCT SYSTEM DESIGN RESPONSE ON 1, 2 or MORE USER STORIES\n",
    "            {design}\n",
    "            \n",
    "            USER INSTRUCTIONS_FOR_CODE_GENERATION\n",
    "            {user_instruction_for_code_generation}\n",
    "            \n",
    "            You might have generated code before and user might have given some feedback for the same. If USER INSTRUCTIONS is\n",
    "            empty assume its first time if not then follow the feedback for regeneration.\n",
    "\n",
    "            USER INSTRUCTIONS_ON_CODE_REGERATION\n",
    "            {user_feedback_on_code}\n",
    "            \n",
    "            \"\"\"\n",
    "        \n",
    "    \n",
    "        code = call_model(self.model,ai_engineer_prompt,self.O3_MINI_MODEL)\n",
    "        await ctx.set(\"code\", str(code))\n",
    "        pevent = ProgressEvent(\n",
    "            msg=\"Code generated successfully\",\n",
    "            event_type=\"code_feedback\"\n",
    "        )\n",
    "        ctx.write_event_to_stream(pevent)\n",
    "        return InputRequiredEvent(\n",
    "            prefix=\"\\n\\n\"+\"Above are the code for selected design . Do you have any feedback please let us know or You can go for DevOps code generation process\",\n",
    "            result=code,\n",
    "            event_type=\"code_feedback\"\n",
    "        )\n",
    "    \n",
    "    \n",
    "    @step\n",
    "    async def get_code_feedback(self, ctx: Context, ev: HumanResponseEvent) -> CodeFeedbackEvent | DevOpsEngineerEvent | StopEvent:\n",
    "        if (ev.event_type == \"code_feedback\"):\n",
    "            response = ev.response\n",
    "            feedback = f\"\"\"\n",
    "                You have received some human feedback on the code. User either can give some feedback and ask\n",
    "                for the regeneration code again or ask for the next step that is DevOps code for design.\n",
    "                <feedback>\n",
    "                {response}\n",
    "                </feedback>\n",
    "                If there's any feedback on code and ask for regenration, respond with just the word 'FEEDBACK'.\n",
    "                If ask for go for DevOps code generation or ask for next step say 'OKAY'.\n",
    "                Any If ask for finish or any other thing say 'CLOSE'.\n",
    "\n",
    "            \"\"\"\n",
    "            result = call_model(self.model,feedback,self.GPT_4O_MINI_MODEL)\n",
    "\n",
    "            verdict = result.strip()\n",
    "            if (verdict == \"OKAY\"):\n",
    "                pevent = ProgressEvent(\n",
    "                    msg=f\"User has provided update on the Code OK. Generating DevOps Code\",\n",
    "                    event_type=\"GENERATE_DEVOPS_CODE\"\n",
    "                )\n",
    "                ctx.write_event_to_stream(pevent)\n",
    "                await ctx.set(\"user_instruction_for_devops_code_generation\", str(response))\n",
    "                return DevOpsEngineerEvent(reviewedCode= await ctx.get(\"code\"))\n",
    "            if (verdict == \"CLOSE\"):\n",
    "                pevent = ProgressEvent(\n",
    "                    msg=f\"User want to exit the workflow. Stopping the Workflow. Please start new workflow by submitting new requirement\",\n",
    "                    event_type=\"STOP\"\n",
    "                )\n",
    "                ctx.write_event_to_stream(pevent)\n",
    "                return StopEvent(result = \"Excellent you have got what you need. Best of luck for your product\")\n",
    "            else:\n",
    "                pevent = ProgressEvent(\n",
    "                    msg=f\"User has provided feedback on the Code. So Generating Code again with feedback\",\n",
    "                    event_type=\"GENERATE_CODE\"\n",
    "                )\n",
    "                ctx.write_event_to_stream(pevent)\n",
    "                return CodeFeedbackEvent(feedback=ev.response)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    @step\n",
    "    async def generateDevOps(self, ctx: Context, ev: DevOpsEngineerEvent | DevOpsFeedbackEvent) -> InputRequiredEvent:\n",
    "        design = await ctx.get(\"design\")\n",
    "        code = await ctx.get(\"code\")\n",
    "        user_instruction_for_devops_code_generation = ctx.get(\"user_instruction_for_devops_code_generation\")\n",
    "        user_feedback_on_devops_code = \"\"\n",
    "        if hasattr(ev,\"feedback\"):\n",
    "            user_feedback_on_devops_code = ev.feedback\n",
    "        \n",
    "        ai_devOps_prompt = f\"\"\"\n",
    "\n",
    "            You are an expert AI DevOps Engineer responsible for automating CI/CD workflows, deployment pipelines, and \n",
    "            infrastructure provisioning. Your tasks include:\n",
    "\n",
    "            1-) CI/CD Pipeline Configuration: Set up GitHub Actions, Jenkins, or other CI/CD tools for automated testing and deployment.\n",
    "            2-) Deployment & Monitoring: Ensure smooth rollouts with proper rollback mechanisms, environment-specific configurations, and monitoring setup.\n",
    "            3-) Infrastructure Automation: Auto-generate Dockerfiles, Kubernetes manifests, Terraform scripts, and Helm charts for cloud deployment.\n",
    "            4-) Security & Compliance: Implement secrets management, RBAC policies, and best practices for secure deployments.\n",
    "            5-) Observability & Alerting: Configure logging, monitoring, and alerting using tools like Prometheus, Grafana, and ELK stack.\n",
    "\n",
    "\n",
    "            You will given the system design for user story and generated code for the same. You will also \n",
    "            given additional user instruction from user which infrastruture, devops tooling, deployemnt policies, \n",
    "            IAC will be used.\n",
    "\n",
    "            DESIGN_FOR_A_USER_STORY\n",
    "            {design}\n",
    "            \n",
    "            CODE_FOR_THE_SYSTEM_DESIGN\n",
    "            {code}\n",
    "\n",
    "            USER INSTRUCTIONS_FOR_DEVOPS_CODE_GENERATION\n",
    "            {user_instruction_for_devops_code_generation}\n",
    "            \n",
    "            You might have generated devops code before and user might have given some feedback for the same. If USER INSTRUCTIONS is\n",
    "            empty assume its first time if not then follow the feedback for regeneration.\n",
    "\n",
    "            USER INSTRUCTIONS_ON_DEVOPS_CODE_REGERATION\n",
    "            {user_feedback_on_devops_code}\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        devops_code = call_model(self.model,ai_devOps_prompt,self.O3_MINI_MODEL)\n",
    "        \n",
    "        await ctx.set(\"devops_code\", str(devops_code))\n",
    "        pevent = ProgressEvent(\n",
    "            msg=\"DevOps Code generated successfully\",\n",
    "            event_type=\"devops_code_feedback\"\n",
    "        )\n",
    "        ctx.write_event_to_stream(pevent)\n",
    "        return InputRequiredEvent(\n",
    "            prefix=\"\\n\\n\"+\"Above are the DevOps code for selected design . Do you have any feedback please let us know or You can go for test cases generation\",\n",
    "            result=devops_code,\n",
    "            event_type=\"devops_code_feedback\"\n",
    "        )\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    @step\n",
    "    async def get_devOps_code_feedback(self, ctx: Context, ev: HumanResponseEvent) -> DevOpsFeedbackEvent | TestEngineerEvent | StopEvent:\n",
    "        if (ev.event_type == \"devops_code_feedback\"):\n",
    "            response = ev.response\n",
    "            feedback = f\"\"\"\n",
    "                You have received some human feedback on the devops code. User either can give some feedback and ask\n",
    "                for the regeneration devops code again or ask for the next step that is test case generation process.\n",
    "                <feedback>\n",
    "                {response}\n",
    "                </feedback>\n",
    "                If there's any feedback on devops code and ask for regenration, respond with just the word 'FEEDBACK'.\n",
    "                If ask for go for Test case generation or ask for next step say 'OKAY'.\n",
    "                Any If ask for finish or any other thing say 'CLOSE'.\n",
    "\n",
    "            \"\"\"\n",
    "            result = call_model(self.model,feedback,self.GPT_4O_MINI_MODEL)\n",
    "\n",
    "            verdict = result.strip()\n",
    "\n",
    "            print(f\"LLM says the devops code feedback verdict was {verdict}\")\n",
    "            if (verdict == \"OKAY\"):\n",
    "                pevent = ProgressEvent(\n",
    "                    msg=f\"User has provided update on the DEVOPS Code OK. Generating Test Cases\",\n",
    "                    event_type=\"GENERATE_TESTCASE\"\n",
    "                )\n",
    "                ctx.write_event_to_stream(pevent)\n",
    "                await ctx.set(\"user_instruction_for_testcase_generation\", str(response))\n",
    "                return TestEngineerEvent(devopscode= await ctx.get(\"devops_code\"))\n",
    "            if (verdict == \"CLOSE\"):\n",
    "                pevent = ProgressEvent(\n",
    "                    msg=f\"User want to exit the workflow. Stopping the Workflow. Please start new workflow by submitting new requirement\",\n",
    "                    event_type=\"STOP\"\n",
    "                )\n",
    "                ctx.write_event_to_stream(pevent)\n",
    "                return StopEvent(result = \"Excellent you have got what you need. Best of luck for your product\")\n",
    "            else:\n",
    "                pevent = ProgressEvent(\n",
    "                    msg=f\"User has provided feedback on the DevOps Code. So Generating DevOps Code again with feedback\",\n",
    "                    event_type=\"GENERATE_DEVOPS_CODE\"\n",
    "                )\n",
    "                ctx.write_event_to_stream(pevent)\n",
    "                return DevOpsFeedbackEvent(feedback=ev.response)\n",
    "    \n",
    "    @step\n",
    "    async def generateTestCases(self, ctx: Context, ev: TestEngineerEvent | TestCaseFeedbackEvent) -> InputRequiredEvent:\n",
    "        design = await ctx.get(\"design\")\n",
    "        code = await ctx.get(\"code\")\n",
    "        devops_configuration = await ctx.get(\"devops_code\")\n",
    "        \n",
    "        user_instruction_for_testcase_generation = ctx.get(\"user_instruction_for_testcase_generation\")\n",
    "        user_feedback_on_testcase_generation = \"\"\n",
    "        if hasattr(ev,\"feedback\"):\n",
    "            user_feedback_on_testcase_generation = ev.feedback\n",
    "            \n",
    "        ai_expert_tester = f\"\"\"\n",
    "            As an AI Expert Tester, your goal is to ensure software reliability, performance, and security by \n",
    "            conducting automated testing across the entire development lifecycle. You will receive design specifications for a user story,\n",
    "            application code for system design, and DevOps configurations as input. \n",
    "\n",
    "            Your Responsibilities:\n",
    "            1-) Analyze the Design, Verify functional and non-functional requirements, Identify potential edge cases, \n",
    "            missing scenarios, and security loopholes.\n",
    "\n",
    "            2-) Review the Code - Generate comprehensive unit tests ensuring high code coverage. Detect logical inconsistencies, error handling issues, and potential regressions.\n",
    "            Validate security best practices (e.g., OWASP Top 10 vulnerabilities).\n",
    "\n",
    "            3-) Test the DevOps Code: Review CI/CD pipelines for robustness and security. Generate load testing scripts to \n",
    "            validate system scalability.\n",
    "\n",
    "            4-) Ensure Dockerfiles, Kubernetes manifests, and Terraform scripts are optimized for deployment if any\n",
    "\n",
    "            5-) Automate the Testing Process: Use appropriate frameworks (Jest, PyTest, JUnit, k6, JMeter, Locust).\n",
    "\n",
    "            6-) Simulate real-world load, stress, and performance conditions.\n",
    "\n",
    "            7-) Integrate tests into the CI/CD pipeline for continuous validation.\n",
    "\n",
    "            8-) Provide Insights & Reports: Generate detailed reports on test coverage, execution results, and detected issues.\n",
    "\n",
    "            9-) Suggest improvements for code quality, performance, and deployment stability.\n",
    "            \n",
    "            You will also given additional user instruction from user for test case generation if any.\n",
    "        \n",
    "\n",
    "            DESIGN_FOR_A_USER_STORY\n",
    "            {design}\n",
    "            \n",
    "            CODE_FOR_THE_SYSTEM_DESIGN\n",
    "            {code}\n",
    "            \n",
    "            DEVOPS_CONFIGURATION\n",
    "            {devops_configuration}\n",
    "            \n",
    "            USER INSTRUCTIONS_FOR_TESTCASE_GENERATION\n",
    "            {user_instruction_for_testcase_generation}\n",
    "            \n",
    "            You might have generated test cases before and user might have given some feedback for the same. If USER INSTRUCTIONS is\n",
    "            empty assume its first time if not then follow the feedback for regeneration.\n",
    "\n",
    "            USER INSTRUCTIONS_ON_TESTCASE_REGERATION\n",
    "            {user_feedback_on_testcase_generation}\n",
    "\n",
    "            \"\"\"\n",
    "        \n",
    "        testcases = call_model(self.model,ai_expert_tester,self.O3_MINI_MODEL)\n",
    "        \n",
    "        \n",
    "        await ctx.set(\"testcases\", str(testcases))\n",
    "        pevent = ProgressEvent(\n",
    "            msg=\"TestCases generated successfully\",\n",
    "            event_type=\"testcase_feedback\"\n",
    "        )\n",
    "        ctx.write_event_to_stream(pevent)\n",
    "        return InputRequiredEvent(\n",
    "            prefix=\"\\n\\n\"+\"Above are the Test cases for selected design and code. Do you have any feedback please let us know or You can exit the workflow\",\n",
    "            result=testcases,\n",
    "            event_type=\"testcase_feedback\"\n",
    "        )\n",
    "    \n",
    "    \n",
    "    @step\n",
    "    async def get_testcase_feedback(self, ctx: Context, ev: HumanResponseEvent) -> TestCaseFeedbackEvent | StopEvent:\n",
    "        if (ev.event_type == \"testcase_feedback\"):\n",
    "            response = ev.response\n",
    "            feedback = f\"\"\"\n",
    "                You have received some human feedback on the test case generation. User either can give some feedback and ask\n",
    "                for the regeneration test cases again or ask for the finish the workflow\n",
    "                <feedback>\n",
    "                {response}\n",
    "                </feedback>\n",
    "                If there's any feedback on test cases generation and ask for regenration, respond with just the word 'FEEDBACK'.\n",
    "                If ask for finish and any other thing say 'CLOSE'.\n",
    "\n",
    "            \"\"\"\n",
    "            result = call_model(self.model,feedback,self.GPT_4O_MINI_MODEL)\n",
    "\n",
    "            verdict = result.strip()\n",
    "            if (verdict == \"CLOSE\"):\n",
    "                pevent = ProgressEvent(\n",
    "                    msg=f\"User has provided fine with the DEVOPS Code OK. Stopping the Workflow\",\n",
    "                    event_type=\"STOP\"\n",
    "                )\n",
    "                ctx.write_event_to_stream(pevent)\n",
    "                return StopEvent(result = \"Excellent you have got what you need. Best of luck for your product\")\n",
    "            else:\n",
    "                pevent = ProgressEvent(\n",
    "                    msg=f\"User has provided feedback on the Test Cases. So Generating TestCases again with feedback\",\n",
    "                    event_type=\"GENERATE_TESTCASE\"\n",
    "                )\n",
    "                ctx.write_event_to_stream(pevent)\n",
    "                return TestCaseFeedbackEvent(feedback=ev.response)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365aeec0",
   "metadata": {},
   "source": [
    "### Workflow Execution  Defines functions to execute the workflow in a background thread and handle user interactions. Key Functions:\n",
    "    run_workflow_thread: Runs the workflow asynchronously in a separate thread.\n",
    "    get_label_for_event: Maps event types to user-friendly labels with icons.\n",
    "    Gradio Callbacks:\n",
    "        start_workflow: Starts the workflow when the user clicks \"Start Workflow.\"\n",
    "        refresh_status: Refreshes the workflow status.\n",
    "        submit_input: Handles user input during the workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771e966a",
   "metadata": {},
   "source": [
    "#### This cell defines global variables to hold streaming updates, such as progress messages, prompts, and user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3886d5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals to hold streaming updates.\n",
    "global_progress = \"\"    # This will accumulate progress messages\n",
    "global_prompt   = \"\"    # This will hold the current prompt info\n",
    "global_current_event = \"\"\n",
    "input_queue = queue.Queue()   # For sending user responses from the UI to the workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e35f520",
   "metadata": {},
   "source": [
    "#### This cell defines a function to run the workflow in a background thread using an asyncio event loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ba5ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function runs workflow “event loop” in a background thread.\n",
    "# It uses a new asyncio event loop to await events.\n",
    "def run_workflow_thread(requirement):\n",
    "    \n",
    "    \n",
    "    # Create and set an event loop BEFORE calling the workflow.\n",
    "    loop = asyncio.new_event_loop()\n",
    "    asyncio.set_event_loop(loop)\n",
    "    \n",
    "    # instantiate your workflow (make sure you have imported your workflow code)\n",
    "    workflow = AIDevXWorkflow(timeout=None, verbose=False)\n",
    "\n",
    "    async def process_events():\n",
    "        global global_progress, global_prompt, global_current_event\n",
    "        try:\n",
    "            handler = workflow.run(query=requirement)\n",
    "            async for ev in handler.stream_events():\n",
    "                if isinstance(ev, ProgressEvent):\n",
    "                    # Append the message to the progress log\n",
    "                    global_progress += ev.msg + \"\\n\"\n",
    "                    global_current_event = ev.event_type\n",
    "                elif isinstance(ev, InputRequiredEvent):\n",
    "                    # Set the prompt info (this may include a design story, code snippet, etc.)\n",
    "                    global_prompt = ev.result + \"\\n\" + ev.prefix\n",
    "                    global_current_event = ev.event_type\n",
    "                    # Wait in a blocking fashion for user input from the queue.\n",
    "                    # (Since we are in a separate thread, this blocking call is acceptable.)\n",
    "                    response = None\n",
    "                    while response is None:\n",
    "                        try:\n",
    "                            # Use a short timeout so we can check repeatedly (alternatively, you could block)\n",
    "                            response = input_queue.get(timeout=0.1)\n",
    "                        except queue.Empty:\n",
    "                            await asyncio.sleep(0.1)\n",
    "                    # Echo user submission into the progress log if desired\n",
    "                    global_progress += \"User responded  : \" + response + \"\\n\"\n",
    "                    # Clear the prompt once an answer is received\n",
    "                    global_prompt = \"\"\n",
    "                    # Send the response event back into your workflow\n",
    "                    handler.ctx.send_event(\n",
    "                        HumanResponseEvent(\n",
    "                            response=response,\n",
    "                            event_type=ev.event_type\n",
    "                        )\n",
    "                    )\n",
    "        except asyncio.CancelledError:\n",
    "            raise\n",
    "        finally:\n",
    "            # Ensure any pending tasks are properly cleaned up\n",
    "            for task in asyncio.all_tasks(loop):\n",
    "                if task is not asyncio.current_task(loop):\n",
    "                    task.cancel()\n",
    "            \n",
    "            # Wait for all tasks to complete\n",
    "            pending = asyncio.all_tasks(loop)\n",
    "            if pending:\n",
    "                await asyncio.gather(*pending, return_exceptions=True)\n",
    "    \n",
    "\n",
    "    # Run the event loop with proper task cleanup\n",
    "    try:\n",
    "        task = loop.create_task(process_events())\n",
    "        loop.run_until_complete(task)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "    finally:\n",
    "        try:\n",
    "            # Cancel any pending tasks\n",
    "            pending = asyncio.all_tasks(loop)\n",
    "            for task in pending:\n",
    "                task.cancel()\n",
    "            # Allow cancelled tasks to complete\n",
    "            if pending:\n",
    "                loop.run_until_complete(\n",
    "                    asyncio.gather(*pending, return_exceptions=True)\n",
    "                )\n",
    "        finally:\n",
    "            loop.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b8b910",
   "metadata": {},
   "source": [
    "#### This cell defines a function to map event types to user-friendly labels with icons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10767630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_for_event(event_type):\n",
    "    \"\"\"Map event types to user-friendly labels with icons\"\"\"\n",
    "    event_labels = {\n",
    "        \"userstory_feedback\": \"📝 Generated User Stories\",\n",
    "        \"GENERATE_USER_STORIES\": \"📝 User Stories Generation....\",\n",
    "        \"design_feedback\": \"🎨 Generated Design Specification\",\n",
    "        \"GENERATE_DESIGN\": \"🎨 Design Specification Generation....\",\n",
    "        \"code_feedback\": \"💻 Generated Code\",\n",
    "        \"GENERATE_CODE\": \"💻 Code Generation....\",\n",
    "        \"testcase_feedback\": \"🧪 Generated Test Cases\",\n",
    "        \"GENERATE_TESTCASE\": \"🧪 Test Cases Generation....\",\n",
    "        \"devops_code_feedback\": \"📋 Generated DevOps Code\",\n",
    "        \"GENERATE_DEVOPS_CODE\": \"📋 DevOps Code Generation....\",\n",
    "        \"STOP\": \"⌛ Waiting for New Flow to Start\",\n",
    "    }\n",
    "    return event_labels.get(event_type, \"📊 Workflow Progress & Output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37f4442",
   "metadata": {},
   "source": [
    "#### This cell defines a callback function to start the workflow when the user clicks \"Start Workflow.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17370a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio callbacks:\n",
    "# a. start_workflow: called when user clicks “Start Workflow”\n",
    "def start_workflow(requirement):\n",
    "    global global_progress, global_prompt\n",
    "    # Reset globals whenever a new run is started\n",
    "    global_progress = \"\"\n",
    "    global_prompt = \"\"\n",
    "    # Start your workflow in a background thread:\n",
    "    t = threading.Thread(target=run_workflow_thread, args=(requirement,))\n",
    "    t.start()\n",
    "    return \"Workflow started…\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4250de",
   "metadata": {},
   "source": [
    "#### This cell defines a callback function to refresh the status of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb32d1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# b. refresh_status: called (by a Refresh button or a client-side timer) to update UI components.\n",
    "def refresh_status():\n",
    "    # Return the current progress log and prompt text.\n",
    "    requirement = \"\"  # Initialize requirement variable\n",
    "    if(global_current_event == \"STOP\"):\n",
    "        requirement: \"\"  \n",
    "    else:\n",
    "        requirement = gr.update()  # Keep requirement as is\n",
    "    return {\n",
    "        status_output: global_progress,\n",
    "        prompt_output: gr.update(\n",
    "            value=global_prompt,\n",
    "            label=get_label_for_event(global_current_event)\n",
    "        ),\n",
    "        requirement_input:requirement\n",
    "        \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11e31fc",
   "metadata": {},
   "source": [
    "#### This cell defines a callback function to handle user input when the \"Submit\" button is clicked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23abc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c. submit_input: when the user enters their response and clicks “Submit”\n",
    "def submit_input(user_input):\n",
    "    # Put the user input in the queue for the workflow thread.\n",
    "    if user_input and user_input.strip() != \"\":\n",
    "        input_queue.put(user_input)\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e52d7b2",
   "metadata": {},
   "source": [
    "####  User Interface (Gradio Integration).   Builds a user-friendly interface using Gradio for interacting with the workflow. Key Components:\n",
    "    Input fields for user requirements and feedback.\n",
    "    Buttons for starting the workflow, refreshing status, and submitting input.\n",
    "    Output areas for displaying progress and results.\n",
    "    Custom CSS:\n",
    "        Enhances the UI with a modern, clean design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07323d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom CSS for a modern DEVX UI with a light background\n",
    "custom_css = \"\"\"\n",
    "    body {\n",
    "        background-color: #f5f5f5;\n",
    "        color: #212121;\n",
    "        font-family: 'Inter', sans-serif;\n",
    "    }\n",
    "    .title {\n",
    "        color: #2e7d32;\n",
    "        text-align: center;\n",
    "        font-size: 26px;\n",
    "        font-weight: bold;\n",
    "        margin-bottom: 15px;\n",
    "    }\n",
    "    .info-box {\n",
    "        background-color: #e8f5e9;\n",
    "        padding: 15px;\n",
    "        border-radius: 10px;\n",
    "        box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1);\n",
    "        margin-bottom: 15px;\n",
    "    }\n",
    "    .button-primary {\n",
    "        background-color: #2e7d32 !important;\n",
    "        color: #ffffff !important;\n",
    "        font-weight: bold !important;\n",
    "        border-radius: 6px !important;\n",
    "    }\n",
    "    .output-box {\n",
    "        font-family: 'Monaco', 'Courier New', monospace;\n",
    "        background-color: #ffffff;\n",
    "        color: #212121;\n",
    "        padding: 15px;\n",
    "        border-radius: 8px;\n",
    "        min-height: 400px;\n",
    "        white-space: pre-wrap;\n",
    "        border: 1px solid #2e7d32;\n",
    "        overflow-y: auto;\n",
    "    }\n",
    "    .center-content {\n",
    "        display: flex;\n",
    "        justify-content: center;\n",
    "        align-items: center;\n",
    "        margin-bottom: 20px;\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "# Build the enhanced Gradio UI\n",
    "with gr.Blocks(theme=gr.themes.Soft(), css=custom_css) as demo:\n",
    "    \n",
    "\n",
    "    gr.Markdown(\"<div class='title'>🚀 DevX AI-Augmented Software Development</div>\")\n",
    "    gr.Markdown(\n",
    "        \"<div class='info-box'>\"\n",
    "        \"<b>Enter a requirement and start the AI-driven development workflow \"\n",
    "        \"You'll see updates (e.g., design, user stories, code, test cases) in the progress panel. \"\n",
    "        \"When required, provide input in the right panel and continue the workflow.</b> \"\n",
    "        \"</div>\"\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            \n",
    "            requirement_input = gr.Textbox(label=\"📌 Requirement\", \n",
    "                                           placeholder=\"Describe your requirement here...\",\n",
    "                                           lines=5)\n",
    "            \n",
    "            start_button = gr.Button(\"🚀 Start Workflow\", elem_classes=\"button-primary\")\n",
    "            \n",
    "            status_output = gr.Textbox(label=\"📢 Streaming of Events\", \n",
    "                                       lines=15,\n",
    "                                       interactive=False, \n",
    "                                       elem_classes=\"output-box\")\n",
    "            refresh_button = gr.Button(\"🔄 Refresh Status\",elem_classes=\"button-primary\")  # Moved below the progress log\n",
    "            \n",
    "        with gr.Column(scale=1):\n",
    "            \n",
    "            prompt_output = gr.Textbox(label=\"📝 DEVX Output\", \n",
    "                                       lines=30, \n",
    "                                       interactive=False, \n",
    "                                       show_label=True,\n",
    "                                       elem_classes=\"output-box\")\n",
    "          \n",
    "            user_input = gr.Textbox(label=\"✍ Your Input\", \n",
    "                                    placeholder=\"Type your response here...\", \n",
    "                                    lines=2)\n",
    "            submit_button = gr.Button(\"✅ Submit\", elem_classes=\"button-primary\")\n",
    "          \n",
    "\n",
    "    # Wire up the buttons:\n",
    "    start_button.click(start_workflow, inputs=requirement_input, outputs=status_output)\n",
    "    refresh_button.click(refresh_status, outputs=[status_output, prompt_output, requirement_input])\n",
    "    submit_button.click(submit_input, inputs=user_input, outputs=user_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7046e9",
   "metadata": {},
   "source": [
    "#### This cell launches the Gradio application, which provides a user interface for interacting with the AI-driven development workflow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd8a28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the Gradio app\n",
    "demo.launch(\n",
    "    share=False, \n",
    "    server_port=9000, \n",
    "    prevent_thread_lock=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255fa133",
   "metadata": {},
   "source": [
    "#### This cell closes the Gradio app if it is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328ab824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587a42d7",
   "metadata": {},
   "source": [
    "#### This section uses the draw_all_possible_flows function to visualize the workflow. It helps in understanding the flow of events and dependencies between different stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73cb455",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_all_possible_flows(\n",
    "    AIDevXWorkflow, \n",
    "    filename=\"../workflows/workflow.html\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0f97d9",
   "metadata": {},
   "source": [
    "####  Helper function (extract_html_content  )to format and display code or HTML outputs in the Jupyter Notebook. These functions enhance the readability of the generated outputs. It renders the workflow diagram generated in the previous step, providing a graphical representation of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af68114e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "def extract_html_content(filename):\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            html_content = file.read()\n",
    "            html_content = f\"\"\" <div style=\"width: 100%; height: 800px; overflow: hidden;\"> {html_content} </div>\"\"\"\n",
    "            return html_content\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error reading file: {str(e)}\")\n",
    "\n",
    "html_content = extract_html_content(\"../workflows/workflow.html\")\n",
    "display(HTML(html_content), metadata=dict(isolated=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc38546d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81efd329-2a0f-4baa-a8d7-8bac39d3afce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
